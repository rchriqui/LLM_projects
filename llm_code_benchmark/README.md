# ğŸš€ LLM Code Benchmark: Python â†’ C++/Rust Performance Comparison

## ğŸ“‹ Description

Ce projet Ã©value et compare les performances de diffÃ©rents **Large Language Models (LLMs)** dans la gÃ©nÃ©ration de code optimisÃ©. L'objectif est de convertir du code Python en C++ (et potentiellement Rust) en utilisant plusieurs modÃ¨les LLM, puis de compiler, exÃ©cuter et comparer les performances pour identifier quel LLM gÃ©nÃ¨re le code le plus performant.

## ğŸ¯ Objectifs

- **GÃ©nÃ©ration de code** : Utiliser diffÃ©rents LLMs pour convertir du code Python en C++
- **Benchmark de performance** : Compiler et exÃ©cuter chaque version gÃ©nÃ©rÃ©e
- **Comparaison** : Classer les LLMs selon la vitesse d'exÃ©cution du code gÃ©nÃ©rÃ©
- **Ã‰valuation** : Mesurer l'amÃ©lioration de performance par rapport au code Python original

## ğŸ”§ Technologies UtilisÃ©es

- **Python** : Code source original et orchestration
- **C++** : Code gÃ©nÃ©rÃ© par les LLMs
- **OpenRouter API** : Interface unifiÃ©e pour accÃ©der Ã  plusieurs LLMs
- **Gradio** : Interface utilisateur pour l'interaction
- **Jupyter Notebook** : Environnement de dÃ©veloppement et d'expÃ©rimentation

## ğŸ¤– LLMs TestÃ©s

Le projet compare les modÃ¨les suivants (classÃ©s selon leurs performances) :

1. **Gemini 3 Pro Preview** (`google/gemini-3-pro-preview`)
2. **GPT-5.2 Codex** (`openai/gpt-5.2-codex`)
3. **Claude Opus 4.6** (`anthropic/claude-opus-4.6`)
4. **Gemini 3 Flash Preview** (`google/gemini-3-flash-preview`)
5. **Kimi K2.5** (`moonshotai/kimi-k2.5`)
6. **GLM-5** (`z-ai/glm-5`)

## ğŸ“ Structure du Projet

```
llm_code_benchmark/
â”œâ”€â”€ python_c_rust.ipynb          # Notebook principal avec l'interface Gradio
â”œâ”€â”€ _bench_python.py              # Code Python de rÃ©fÃ©rence pour le benchmark
â”œâ”€â”€ _verify_cpp.cpp               # Code C++ de vÃ©rification
â”œâ”€â”€ _verify_cpp_exe               # ExÃ©cutable compilÃ©
â”œâ”€â”€ main.cpp                      # Template C++ principal
â”œâ”€â”€ system_info.py                # Script pour obtenir les infos systÃ¨me
â”œâ”€â”€ requirements.txt              # DÃ©pendances Python
â”œâ”€â”€ generated_*.cpp               # Code C++ gÃ©nÃ©rÃ© par chaque LLM
â”œâ”€â”€ main_*                        # ExÃ©cutables compilÃ©s pour chaque modÃ¨le
â””â”€â”€ __pycache__/                  # Cache Python
```

## ğŸš€ Installation

1. **Cloner le repository** (si applicable)

2. **Installer les dÃ©pendances** :
```bash
pip install -r requirements.txt
```

3. **Configurer les variables d'environnement** :
   - CrÃ©er un fichier `.env` Ã  la racine du projet `LLM_projects`
   - Ajouter votre clÃ© API OpenRouter :
   ```
   OPENROUTER_API_KEY=sk-or-votre-cle-ici
   ```

4. **Compiler les exÃ©cutables C++** (si nÃ©cessaire) :
```bash
g++ -O3 -o main main.cpp
```

## ğŸ’» Utilisation

1. **Lancer le notebook Jupyter** :
```bash
jupyter notebook python_c_rust.ipynb
```

2. **ExÃ©cuter toutes les cellules** pour initialiser l'interface Gradio

3. **Utiliser l'interface Gradio** pour :
   - SÃ©lectionner un modÃ¨le LLM
   - GÃ©nÃ©rer du code C++ Ã  partir du code Python
   - Compiler et exÃ©cuter automatiquement
   - Comparer les performances

## ğŸ“Š RÃ©sultats

Le projet gÃ©nÃ¨re des mÃ©triques de performance incluant :
- Temps d'exÃ©cution de chaque version C++
- Comparaison avec le code Python original
- Classement des LLMs par performance du code gÃ©nÃ©rÃ©

## ğŸ” Exemple de Code TestÃ©

Le benchmark utilise un calcul intensif avec des boucles pour mesurer les performances :

```python
def calculate(iterations, param1, param2):
    result = 1.0
    for i in range(1, iterations+1):
        j = i * param1 - param2
        result -= (1/j)
        j = i * param1 + param2
        result += (1/j)
    return result
```

## ğŸ“ Notes

- Les rÃ©sultats peuvent varier selon le matÃ©riel utilisÃ©
- Les performances dÃ©pendent de la qualitÃ© de la compilation C++
- Le projet utilise des optimisations de compilation (`-O3`) pour maximiser les performances

## ğŸ¤ Contribution

Ce projet fait partie d'une sÃ©rie d'expÃ©rimentations sur l'utilisation des LLMs pour la gÃ©nÃ©ration et l'optimisation de code.

## ğŸ“„ Licence

Voir le fichier LICENSE Ã  la racine du projet.
