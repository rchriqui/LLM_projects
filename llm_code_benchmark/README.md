# ğŸš€ LLM Code Benchmark: Python â†’ C++/Rust Performance Comparison

## ğŸ“‹ Description

This project evaluates and compares the performance of different **Large Language Models (LLMs)** in generating optimized code. The goal is to convert Python code to C++ (and potentially Rust) using multiple LLM models, then compile, execute, and compare performance to identify which LLM generates the most performant code.

## ğŸ¯ Objectives

- **Code Generation**: Use different LLMs to convert Python code to C++
- **Performance Benchmarking**: Compile and execute each generated version
- **Comparison**: Rank LLMs based on the execution speed of generated code
- **Evaluation**: Measure performance improvement compared to the original Python code

## ğŸ”§ Technologies Used

- **Python**: Original source code and orchestration
- **C++**: Code generated by LLMs
- **OpenRouter API**: Unified interface to access multiple LLMs
- **Gradio**: User interface for interaction
- **Jupyter Notebook**: Development and experimentation environment

## ğŸ¤– Tested LLMs

The project compares the following models (ranked by their performance):

1. **Gemini 3 Pro Preview** (`google/gemini-3-pro-preview`)
2. **GPT-5.2 Codex** (`openai/gpt-5.2-codex`)
3. **Claude Opus 4.6** (`anthropic/claude-opus-4.6`)
4. **Gemini 3 Flash Preview** (`google/gemini-3-flash-preview`)
5. **Kimi K2.5** (`moonshotai/kimi-k2.5`)
6. **GLM-5** (`z-ai/glm-5`)

## ğŸ“ Project Structure

```
llm_code_benchmark/
â”œâ”€â”€ python_c_rust.ipynb          # Main notebook with Gradio interface
â”œâ”€â”€ _bench_python.py              # Reference Python code for benchmarking
â”œâ”€â”€ _verify_cpp.cpp               # C++ verification code
â”œâ”€â”€ _verify_cpp_exe               # Compiled executable
â”œâ”€â”€ main.cpp                      # Main C++ template
â”œâ”€â”€ system_info.py                # Script to get system information
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ generated_*.cpp               # C++ code generated by each LLM
â”œâ”€â”€ main_*                        # Compiled executables for each model
â””â”€â”€ __pycache__/                  # Python cache
```

## ğŸš€ Installation

1. **Clone the repository** (if applicable)

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Configure environment variables**:
   - Create a `.env` file at the root of the `LLM_projects` project
   - Add your OpenRouter API key:
   ```
   OPENROUTER_API_KEY=sk-or-your-key-here
   ```

4. **Compile C++ executables** (if needed):
```bash
g++ -O3 -o main main.cpp
```

## ğŸ’» Usage

1. **Launch Jupyter Notebook**:
```bash
jupyter notebook python_c_rust.ipynb
```

2. **Execute all cells** to initialize the Gradio interface

3. **Use the Gradio interface** to:
   - Select an LLM model
   - Generate C++ code from Python code
   - Automatically compile and execute
   - Compare performance

## ğŸ“Š Results

The project generates performance metrics including:
- Execution time of each C++ version
- Comparison with the original Python code
- Ranking of LLMs by performance of generated code

## ğŸ” Example Test Code

The benchmark uses an intensive calculation with loops to measure performance:

```python
def calculate(iterations, param1, param2):
    result = 1.0
    for i in range(1, iterations+1):
        j = i * param1 - param2
        result -= (1/j)
        j = i * param1 + param2
        result += (1/j)
    return result
```

## ğŸ“ Notes

- Results may vary depending on the hardware used
- Performance depends on the quality of C++ compilation
- The project uses compilation optimizations (`-O3`) to maximize performance

## ğŸ¤ Contribution

This project is part of a series of experiments on using LLMs for code generation and optimization.

## ğŸ“„ License

See the LICENSE file at the root of the project.
